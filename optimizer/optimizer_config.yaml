base_optimizer:
  learning_rate: 0.2
  weight_decay: 0

adam_optimizer:
  betas: [0.9, 0.999]
  eps: 1e-08

sgd_optimizer:
  momentum: 0.9
